
from selenium import webdriver
from scrapy import Selector
#模拟鼠标ActionChains
from selenium.webdriver.common.action_chains import ActionChains
import time,datetime

#小说下载
driver =webdriver.Firefox(executable_path="D:/PycharmProjects/geckodriver.exe")
driver.get("第一章开始的网址")
t_selector = Selector(text=driver.page_source)

filename='xxx.txt'
#获取下一页href路径需要判断的值
next_pagehref=t_selector.css("#j_chapterNext::text").extract_first()
#获取当前时间
now=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print(now)
#关闭小窗口
next_page=driver.find_element_by_css_selector("#j_closeGuide")
ActionChains(driver).click(next_page).perform()
#设置一个值让判断循环多少次
i=0
l={0}

#循环下一页
while next_pagehref:
    try:
        #重新get
        driver.get(driver.current_url)
        #下拉滚动条
        driver.execute_script("window.scrollTo(0,document.body.scrollHeight); var lenOfpage=document.body.scrollHeight; return lenOfpage")
        #重新获取页面信息
        t_selector = Selector(text=driver.page_source)
        #要获取的文本信息
        t_head = t_selector.css(".j_chapterName::text").extract_first()
        t_list = t_selector.css(".read-content p::text").extract()
        for x in l:
            i+=1
        #写入文件 a插入
        with open(filename,'a',encoding='utf-8')as file_object:
            file_object.write(t_head + '\n')
            #t_list 需要遍历
            for t_l in t_list:
                file_object.write(t_l + '\n')
        time.sleep(0.5)

        #定位下一页元素
        next_page = driver.find_element_by_css_selector("#j_chapterNext")
        #模拟鼠标单击左键
        ActionChains(driver).click(next_page).perform()
        time.sleep(0.3)
        print(driver.current_url)
        #判断循环次数，清理cookies
        if i>100:
            driver.delete_all_cookies()
            i=0
        if next_pagehref == "书末页":
            break
    except TypeError:
        if (TypeError):
            print (now+driver.current_url)
            break

print(i)
print(now)
driver.quit()


